<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on hwholiday</title>
    <link>https://hwholiday.github.io/categories/kubernetes/</link>
    <description>Recent content in Kubernetes on hwholiday</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 31 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://hwholiday.github.io/categories/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Filebeat 收集 K8S 日志</title>
      <link>https://hwholiday.github.io/2021/k8s-filebeat/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://hwholiday.github.io/2021/k8s-filebeat/</guid>
      <description>介绍 每个Node节点上的容器应用日志，默认都会在/var/log/containers （标准输出） Pod -&amp;gt; /var/log/containers/*.log -&amp;gt; Filebeat -&amp;gt; Logstash -&amp;gt; 其他输出 app-*.log 是你的应用日志文件（可在宿主机的/var/log/containers/目录下查看自己的日志格式） 使用DaemonSet安装 filebeat （filebeat.yaml） apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: filebeat name: filebeat-daemonset spec: selector: matchLabels: app: filebeat template: metadata: labels: app: filebeat spec: containers: - name: filebeat-daemonset image: docker.elastic.co/beats/filebeat:7.12.1 args: [ &amp;#34;-c&amp;#34;, &amp;#34;/usr/share/filebeat/filebeat.yml&amp;#34;, &amp;#34;-e&amp;#34;, ] securityContext: runAsUser: 0 volumeMounts: - mountPath: /usr/share/filebeat/filebeat.yml name: volume-configmap subPath: filebeat.yml readOnly: true - mountPath: /data/app/docker/containers name: varlibdockercontainers readOnly: true - name: varlog mountPath: /var/log readOnly: true volumes: - name: volume-configmap configMap: defaultMode: 420 name: filebeat-configmap - name: varlibdockercontainers hostPath: path: /data/app/docker/containers - name: varlog hostPath: path: /var/log --- apiVersion: v1 data: filebeat.</description>
    </item>
    
    <item>
      <title>Kubernetes 集群挂载高可用 NFS 网络文件存储</title>
      <link>https://hwholiday.github.io/2021/k8s-nsf/</link>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://hwholiday.github.io/2021/k8s-nsf/</guid>
      <description>安装NFS高可用集群 安装 nfs-client-provisioner curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh helm repo add azure http://mirror.azure.cn/kubernetes/charts/ helm install nfs-storage azure/nfs-client-provisioner --set nfs.server=&amp;#34;172.12.17.200&amp;#34; --set nfs.path=/fs/nsf-data --set storageClass.name=nfs-hw 创建PVC kubectl apply -f nfs-pvc.yaml
apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-pvc-test spec: storageClassName: &amp;#34;nfs-hw&amp;#34; accessModes: - ReadWriteMany resources: requests: storage: 10Gi 测试 kubectl apply -f nfs-test.yaml
apiVersion: batch/v1 kind: Job metadata: name: nfsjob spec: template: spec: containers: - name: bbox1 image: busybox args: - /bin/sh - -c - echo &amp;#34;1231231231&amp;#34; &amp;gt; /nfs-data/hello volumeMounts: - mountPath: &amp;#34;/nfs-data&amp;#34; name: nfsdata restartPolicy: Never volumes: - name: nfsdata persistentVolumeClaim: claimName: nfs-pvc-test end 接下来去看看 NFS 服务器上是否有对应文件文件 K8S 集群是 v1.</description>
    </item>
    
    <item>
      <title>NFS (Kubernetes) 高可用方案（NFS&#43;keepalived&#43;Sersync)</title>
      <link>https://hwholiday.github.io/2021/nsf/</link>
      <pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://hwholiday.github.io/2021/nsf/</guid>
      <description>简介 本方案 NFS 的高可用方案，客户端为 k8s集群 ，三台文件服务器分别Master和 Slave，使用 keepalived 生成一个虚拟 IP，使用 Sersync 进行 Master 与 Slave 之间文件相互同步，确保高可用。
角色 系统 IP 虚拟IP 172.12.17.200 Client k8s集群 172.12.17.163,172.12.17.164,172.12.17.165 Master centos 7.5 172.12.17.166 Slave centos 7.5 172.12.17.167 查看内核版本 uname -smr 建议升级到最新的长期支持版本 升级内核
安装nfs 在master和slave上面创建共享目录 mkdir -p /fs/nsf-data 在master和slave上安装nfs服务 // 下载nfs yum -y install nfs-utils rpcbind //写入配置（配置要改动需要reload nfs 服务） echo &amp;#39;/fs/nsf-data 172.12.17.163 172.12.17.164 172.12.17.165(rw,sync,all_squash)&amp;#39; &amp;gt;&amp;gt; /etc/exports //开启服务 systemctl start rpcbind &amp;amp;&amp;amp; systemctl start nfs //设置开机自启动 systemctl enable rpcbind &amp;amp;&amp;amp; systemctl enable nfs 在k8s集群里面找个node 测试是否能链接上nfs服务 // 下载nfs yum -y install nfs-utils rpcbind // 挂载目录 mkdir /test-nsf-data // 挂载nfs mount -t nfs 172.</description>
    </item>
    
    <item>
      <title>安装 Rancher 控制面板到 K8s 集群</title>
      <link>https://hwholiday.github.io/2021/install_rancher_rke2/</link>
      <pubDate>Wed, 03 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://hwholiday.github.io/2021/install_rancher_rke2/</guid>
      <description>Helm 一键安装脚本
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash cert-manager 证书 kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.4/cert-manager.crds.yaml kubectl create namespace cert-manager helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager --namespace cert-manager --version v1.0.4 kubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-756bb56c5-gl7j9 1/1 Running 0 87s cert-manager-cainjector-86bc6dc648-sv897 1/1 Running 0 87s cert-manager-webhook-66b555bb5-2q996 1/1 Running 0 87s 安装 Rancher kubectl create namespace rancher-system helm repo add rancher-stable https://releases.rancher.com/server-charts/stable helm install rancher rancher-stable/rancher --namespace rancher-system --set hostname=rancher.</description>
    </item>
    
    <item>
      <title>安装 k8s 集群 Rancher RKE</title>
      <link>https://hwholiday.github.io/2021/install_rancher_rke1/</link>
      <pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://hwholiday.github.io/2021/install_rancher_rke1/</guid>
      <description>内核 K8S 1.18开始使用了IPVS 所以4.x以下的内核无法再运行K8S 网络会有BUG 官方推荐4.19LTS及以上的内核
查看内核版本 uname -smr 建议升级到最新的长期支持版本
升级内核 安装 Docker yum install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install docker-ce-19.03.9 docker-ce-cli-19.03.9 containerd.io 启动 Docker systemctl enable docker systemctl start docker 安装 Rancher RKE 禁用 SELinux /usr/sbin/sestatus -v |grep &amp;#34;SELinux status&amp;#34; #结果为 enabled 为启用状态 vim /etc/selinux/config 将SELINUX=enforcing改为SELINUX=disabled 禁用 swap free -h #total used free shared buff/cache available #Mem: 7.8G 205M 6.9G 8.7M 715M 7.3G #Swap: 5.0G 0B 5.0G #Swap 有值代表启用了swap vim /etc/fstab 使用 # 注释掉有 swap 的一行 关闭防火墙 firewall-cmd --state systemctl stop firewalld.</description>
    </item>
    
    <item>
      <title>通过 istio 部署ws,grpc服务并链接外部redis集群</title>
      <link>https://hwholiday.github.io/2021/istio/</link>
      <pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://hwholiday.github.io/2021/istio/</guid>
      <description>安装 docker https://docs.docker.com/engine/install/ 安装单节点 k8s 环境 sudo docker run --privileged -d --restart=unless-stopped -p 88:80 -p 433:443 rancher/rancher 访问 https://宿主机IP:433 新建集群单结点要把 etcd ， control plane ，worker 都给勾选上 复制命令启动，等待就好了 软件架构 准备镜像 网关：ws 服务 在目录 gateway 下 服务 V1:grpc 服务 在目录 logic_v1 下 服务 V2:grpc 服务 在目录 logic_v2 下 服务 V3:grpc 服务 在目录 logic_v3 下 链接了 Reids 集群 关于k8s和istio的配置文件 在目录 kube 下
├── gateway.yaml 部署 gateway(Deployment[k8s]) 服务和对应的 Service(k8s) ├── logic.yaml 部署 logic(Deployment[k8s]) 服务和对应的 Service(k8s) ├── net-gateway.</description>
    </item>
    
    <item>
      <title>一键安装 K8s 以及在K8s上部署Go服务</title>
      <link>https://hwholiday.github.io/2020/k8s_v1/</link>
      <pubDate>Fri, 13 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hwholiday.github.io/2020/k8s_v1/</guid>
      <description>通过Rancher一键部署k8s服务 地址: https://rancher.com/quick-start/ 命令 sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher 然后访问你的docker机器的IP地址就可以看到一个叫local的k8s环境，不过不知道是不是我操作不对,我不能使用这个集群 点击添加集群,自定义一通默认到最后给你一个docker命令执行后我们就又得到一个自定义的k8s集群了（弄一个单节点集群的话最后一步把3个都要勾选上） 下载kubectl https://kubernetes.io/docs/tasks/tools/install-kubectl/ 创建一个～/.kube/config文件，把https://k8s集群部署IP/c/local/monitoring 里面的Kubeconfig File按钮里面的文本拷贝进去OK 部署一个 nginx nginx.yaml
apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 运行 &amp;gt;kubectl apply -f nginx.yml deployment.apps/nginx configured &amp;gt;kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7f4768c97b-69t65 1/1 Running 0 105s //删除 deployment kubectl delete deployment nginx-deployment //删除 pod kubectl.</description>
    </item>
    
  </channel>
</rss>
